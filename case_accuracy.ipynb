{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24be3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58c8b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecfda36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demolished</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just demolished a Snowball ??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loud%20bang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't laugh out loud at many things. But man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drought</td>\n",
       "      <td>San Francisco ; CA</td>\n",
       "      <td>ItÛªs time to do away with drought.Check out ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apocalypse</td>\n",
       "      <td>Harlingen; TX</td>\n",
       "      <td>My niece just asked me 'would you be scared if...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bridge%20collapse</td>\n",
       "      <td>Mumbai ; India</td>\n",
       "      <td>Warne shocked over Australia's epic collapse a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>what time does your talk go until? I don't kno...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>ruin</td>\n",
       "      <td>London / Birmingham</td>\n",
       "      <td>Im so anxious though because so many ppl will ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>whirlwind</td>\n",
       "      <td>pettyville; usa</td>\n",
       "      <td>this week has been a whirlwind but this  is ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>rescue</td>\n",
       "      <td>Toronto; Ontario</td>\n",
       "      <td>UD: Rescue (Structural Collapse) - Scott Road ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>drowning</td>\n",
       "      <td>Coventry</td>\n",
       "      <td>Why are you drowning in low self-image? Take t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                keyword             location  \\\n",
       "0            demolished                  NaN   \n",
       "1           loud%20bang                  NaN   \n",
       "2               drought   San Francisco ; CA   \n",
       "3            apocalypse        Harlingen; TX   \n",
       "4     bridge%20collapse       Mumbai ; India   \n",
       "...                 ...                  ...   \n",
       "996              ablaze        San Francisco   \n",
       "997                ruin  London / Birmingham   \n",
       "998           whirlwind      pettyville; usa   \n",
       "999              rescue     Toronto; Ontario   \n",
       "1000           drowning             Coventry   \n",
       "\n",
       "                                                   text  target  \n",
       "0                         Just demolished a Snowball ??       0  \n",
       "1     I don't laugh out loud at many things. But man...       0  \n",
       "2     ItÛªs time to do away with drought.Check out ...       1  \n",
       "3     My niece just asked me 'would you be scared if...       0  \n",
       "4     Warne shocked over Australia's epic collapse a...       1  \n",
       "...                                                 ...     ...  \n",
       "996   what time does your talk go until? I don't kno...       0  \n",
       "997   Im so anxious though because so many ppl will ...       0  \n",
       "998   this week has been a whirlwind but this  is ex...       0  \n",
       "999   UD: Rescue (Structural Collapse) - Scott Road ...       1  \n",
       "1000  Why are you drowning in low self-image? Take t...       0  \n",
       "\n",
       "[1001 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04239289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7263681592039801\n",
      "Test score: 72.64 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle \n",
    "\n",
    "# Загрузка датасета из файла\n",
    "df = pd.read_csv('train_full.csv')\n",
    "\n",
    "# Создание списка признаков на основе текста сообщения\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Определение целевой переменной\n",
    "y = df['target']\n",
    "\n",
    "# Разделение выборки на обучающую и тестовую\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучение модели логистической регрессии\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Оценка качества модели на тестовой выборке\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Save to file in the current working directory \n",
    "pkl_filename = \"pickle_model.pkl\" \n",
    "with open(pkl_filename, 'wb') as file: \n",
    "        pickle.dump(model, file) \n",
    " \n",
    " # Load from file \n",
    "with open(pkl_filename, 'rb') as file: \n",
    "    pickle_model = pickle.load(file) \n",
    " \n",
    " # Calculate the accuracy score and predict target values \n",
    "score = pickle_model.score(X_test, y_test) \n",
    "print(\"Test score: {0:.2f} %\".format(100 * score)) \n",
    "Ypredict = pickle_model.predict(X_test) \n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eca4e654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.088*\"collided\" + 0.088*\"wreck\" + 0.079*\"collision\" + 0.062*\"burning\" + 0.062*\"wild%20fires\" + 0.062*\"damage\" + 0.053*\"structural%20failure\" + 0.053*\"accident\" + 0.053*\"obliteration\" + 0.044*\"hazard\"\n",
      "Topic: 1 \n",
      "Words: 0.096*\"body%20bag\" + 0.084*\"burned\" + 0.084*\"wrecked\" + 0.072*\"thunderstorm\" + 0.072*\"crushed\" + 0.072*\"armageddon\" + 0.060*\"devastated\" + 0.048*\"hijacker\" + 0.048*\"injured\" + 0.048*\"siren\"\n",
      "Topic: 2 \n",
      "Words: 0.100*\"hail\" + 0.084*\"derailed\" + 0.084*\"upheaval\" + 0.075*\"sinkhole\" + 0.067*\"windstorm\" + 0.059*\"obliterated\" + 0.059*\"refugees\" + 0.051*\"whirlwind\" + 0.050*\"injury\" + 0.050*\"emergency%20services\"\n",
      "Topic: 3 \n",
      "Words: 0.101*\"tornado\" + 0.079*\"hijacking\" + 0.079*\"rescued\" + 0.079*\"dust%20storm\" + 0.068*\"landslide\" + 0.068*\"rescue\" + 0.057*\"nuclear%20disaster\" + 0.057*\"army\" + 0.046*\"blaze\" + 0.035*\"fear\"\n",
      "Topic: 4 \n",
      "Words: 0.073*\"flattened\" + 0.058*\"demolition\" + 0.051*\"blight\" + 0.044*\"curfew\" + 0.044*\"tsunami\" + 0.044*\"thunder\" + 0.044*\"screaming\" + 0.044*\"drowned\" + 0.037*\"forest%20fires\" + 0.037*\"blazing\"\n",
      "Topic: 5 \n",
      "Words: 0.082*\"hazardous\" + 0.066*\"razed\" + 0.066*\"trouble\" + 0.066*\"oil%20spill\" + 0.066*\"panic\" + 0.058*\"inundated\" + 0.058*\"first%20responders\" + 0.058*\"deluge\" + 0.058*\"collapsed\" + 0.050*\"snowstorm\"\n",
      "Topic: 6 \n",
      "Words: 0.087*\"collide\" + 0.074*\"emergency\" + 0.074*\"wreckage\" + 0.067*\"drowning\" + 0.054*\"crashed\" + 0.047*\"trapped\" + 0.047*\"apocalypse\" + 0.047*\"engulfed\" + 0.040*\"quarantine\" + 0.040*\"attack\"\n",
      "Topic: 7 \n",
      "Words: 0.093*\"mudslide\" + 0.077*\"debris\" + 0.077*\"sandstorm\" + 0.060*\"storm\" + 0.060*\"ruin\" + 0.060*\"derailment\" + 0.060*\"explode\" + 0.051*\"collapse\" + 0.051*\"sunk\" + 0.051*\"heat%20wave\"\n",
      "Topic: 8 \n",
      "Words: 0.090*\"meltdown\" + 0.081*\"evacuation\" + 0.072*\"drought\" + 0.072*\"injuries\" + 0.064*\"traumatised\" + 0.063*\"exploded\" + 0.055*\"desolation\" + 0.055*\"wildfire\" + 0.046*\"emergency%20plan\" + 0.037*\"desolate\"\n",
      "Topic: 9 \n",
      "Words: 0.088*\"bridge%20collapse\" + 0.088*\"quarantined\" + 0.080*\"hurricane\" + 0.064*\"twister\" + 0.056*\"flooding\" + 0.049*\"screamed\" + 0.049*\"blizzard\" + 0.041*\"blew%20up\" + 0.040*\"trauma\" + 0.033*\"sinking\"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# создание словаря для корпуса\n",
    "texts = []\n",
    "for keyword in df['keyword']:\n",
    "    if isinstance(keyword, str):\n",
    "        words = keyword.lower().split()\n",
    "        texts.append(words)\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# создание корпуса\n",
    "corpus = []\n",
    "for keyword in df['keyword']:\n",
    "    if isinstance(keyword, str):\n",
    "        words = keyword.lower().split()\n",
    "        corpus.append(dictionary.doc2bow(words))\n",
    "\n",
    "# создание тематической модели\n",
    "num_topics = 10\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "# вывод топ-5 слов для каждой темы\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bfb5dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: collided wreck collision burning wildfires damage structuralfailure accident obliteration hazard\n",
      "Topic: 1 \n",
      "Words: bodybag burned wrecked thunderstorm crushed armageddon devastated hijacker injured siren\n",
      "Topic: 2 \n",
      "Words: hail derailed upheaval sinkhole windstorm obliterated refugees whirlwind injury emergencyservices\n",
      "Topic: 3 \n",
      "Words: tornado hijacking rescued duststorm landslide rescue nucleardisaster army blaze fear\n",
      "Topic: 4 \n",
      "Words: flattened demolition blight curfew tsunami thunder screaming drowned forestfires blazing\n",
      "Topic: 5 \n",
      "Words: hazardous razed trouble oilspill panic inundated firstresponders deluge collapsed snowstorm\n",
      "Topic: 6 \n",
      "Words: collide emergency wreckage drowning crashed trapped apocalypse engulfed quarantine attack\n",
      "Topic: 7 \n",
      "Words: mudslide debris sandstorm storm ruin derailment explode collapse sunk heatwave\n",
      "Topic: 8 \n",
      "Words: meltdown evacuation drought injuries traumatised exploded desolation wildfire emergencyplan desolate\n",
      "Topic: 9 \n",
      "Words: bridgecollapse quarantined hurricane twister flooding screamed blizzard blewup trauma sinking\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# вывод топ-10 слов для каждой темы\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    words = re.sub(r'[^a-zA-Z\\s]', '', topic).split()\n",
    "    words = [word for word in words if len(word) > 1]  # удалить однобуквенные слова\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, ' '.join(words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
